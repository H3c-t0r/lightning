{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rapid prototyping - Auto.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR4_BAUYs3Mb"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/docs/source/_static/images/logo.png\" alt=\"PyTorch Lightning\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7XbLCXGkll9"
      },
      "source": [
        "# Rapid prototyping notebook\n",
        "Use this to prototype quick ideas, then move to a script to scale up!\n",
        "\n",
        "[Remember! we're always available for support on Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-12iz3cds1-uyyyBYJLiaL2bqVmMN7n~A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LODD6w9ixlT"
      },
      "source": [
        "---\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7-Gg69kMnG"
      },
      "source": [
        "%%capture\n",
        "! pip install -U pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4_TYnt_keJi"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.functional import accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrJDukwPtUnS"
      },
      "source": [
        "---\n",
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, *shapes, num_samples=10000):\n",
        "        super().__init__()\n",
        "        self.shapes = shapes\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        samples = []\n",
        "        for shape in self.shapes:\n",
        "            sample = torch.rand(*shape)\n",
        "            samples.append(sample)\n",
        "\n",
        "        return samples"
      ],
      "metadata": {
        "id": "r36MVF7o9RtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI4HrMertiwO"
      },
      "source": [
        "train = DummyDataset((1, 28, 28), (1,))\n",
        "train = DataLoader(train, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbHFZGD6uF_d"
      },
      "source": [
        "val = DummyDataset((1, 28, 28), (1,))\n",
        "val = DataLoader(val, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ZxtVpJuKEt"
      },
      "source": [
        "test = DummyDataset((1, 28, 28), (1,))\n",
        "test = DataLoader(test, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHpyMPKFkVbZ"
      },
      "source": [
        "---\n",
        "\n",
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ELesz1kVQo"
      },
      "source": [
        "class LitAutoEncoder(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\n",
        "        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # ---------------------------\n",
        "        # REPLACE WITH YOUR OWN LOGIC\n",
        "\n",
        "        x, y = batch\n",
        "        x = x.view(x.size(0), -1)\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "        # --------------------------\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # ---------------------------\n",
        "        # REPLACE WITH YOUR OWN LOGIC\n",
        "\n",
        "        x, y = batch\n",
        "        x = x.view(x.size(0), -1)\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        self.log('val_loss', loss)\n",
        "        # --------------------------\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # ---------------------------\n",
        "        # REPLACE WITH YOUR OWN LOGIC\n",
        "\n",
        "        x, y = batch\n",
        "        x = x.view(x.size(0), -1)\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        loss = F.mse_loss(x_hat, x)\n",
        "        self.log('test_loss', loss)\n",
        "        # --------------------------\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubvW3LGSupmt"
      },
      "source": [
        "---\n",
        "## Train\n",
        "NOTE: in colab, set progress_bar_refresh_rate high or the screen will freeze because of the rapid tqdm update speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dk6Ykv8lI7X"
      },
      "source": [
        "# init model\n",
        "ae = LitAutoEncoder()\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = pl.Trainer(devices=1, accelerator='gpu', max_epochs=5)\n",
        "\n",
        "# Train the model âš¡\n",
        "trainer.fit(ae, train_dataloaders=train, val_dataloaders=val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Q-zDY_vWWq"
      },
      "source": [
        "---\n",
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isX5nR0AvVUv"
      },
      "source": [
        "trainer.test(ae, dataloaders=test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuRYlMvVv3Q-"
      },
      "source": [
        "---\n",
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgmkxSSqv6tk"
      },
      "source": [
        "# Start tensorboard.\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flyi--SpvsJN"
      },
      "source": [
        "---\n",
        "## Observations\n",
        "Do your analysis and notes here!"
      ]
    }
  ]
}
